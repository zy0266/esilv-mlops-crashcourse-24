{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Part 4 - Experiment Tracking</h1>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiment Tracking and Model Management with MLFlow"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are many ways to use the MLFlow Tracking API. For simple local uses, the best is to leave the data management to MLFlow and let it store runs, metrics, models and artifacts locally. For more advanced usage, all of this information can be stored in databases. You can find the detailed on MLFlow's documentation [here](https://mlflow.org/docs/latest/tracking.html#scenario-1-mlflow-on-localhost)."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scenario 1: A single data scientist participating in an ML competition\n",
    "\n",
    "MLflow setup:\n",
    "* Tracking server: no\n",
    "* Backend store: local filesystem\n",
    "* Artifacts store: local filesystem\n",
    "\n",
    "The experiments can be explored locally by launching the MLflow UI."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's print the tracking server URI, where the experiments and runs are going to be logged. We observe it refers to a local path."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "mlflow.set_tracking_uri(\"http://127.0.0.1:5000/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tracking URI: 'http://127.0.0.1:5000/'\n"
     ]
    }
   ],
   "source": [
    "import mlflow\n",
    "\n",
    "print(f\"tracking URI: '{mlflow.get_tracking_uri()}'\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After this initialization, we can connect create a client to connect to the API and see what experiments are present."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By refering to mlflow's [documentation](https://mlflow.org/docs/latest/python_api/mlflow.client.html), create a client and display a list of the available experiments using the search_experiments function. This function could prove useful later to programatically explore experiments (rather than in the UI)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mlflow\n",
    "import mlflow.sklearn\n",
    "\n",
    "# Example: logging an experiment\n",
    "with mlflow.start_run():\n",
    "    mlflow.log_param(\"param1\", 5)\n",
    "    mlflow.log_metric(\"metric1\", 0.86)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<Experiment: artifact_location='mlflow-artifacts:/936627379886763873', creation_time=1736121789416, experiment_id='936627379886763873', last_update_time=1736121789416, lifecycle_stage='active', name='MLflow_track_diamonds', tags={}>,\n",
       " <Experiment: artifact_location='mlflow-artifacts:/0', creation_time=1736121668381, experiment_id='0', last_update_time=1736121668381, lifecycle_stage='active', name='Default', tags={}>]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from mlflow.tracking import MlflowClient\n",
    "\n",
    "client = MlflowClient()\n",
    "\n",
    "experiments = client.search_experiments()\n",
    "experiments"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that there is a default experiment for which the runs are stored locally in the mlruns folder."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating an experiment and logging a new run"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An experiment is a logical entity regrouping the logs of multiple attempts at solving a same problem, called runs. \\\n",
    "We will now work with the classic sklearn dataset iris. Our goal here is to manage to classify the different iris species. To track our models performance, we will log every attempt as a \"run\" and create a new experiment \"iris-experiment-1\" to regroup them."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lookup the mlflow.run and mlflow.start_run functions [here](https://mlflow.org/docs/latest/python_api/mlflow.html?highlight=start_run#mlflow.start_run) to find out how to manage runs.\n",
    "Explore [this part](https://mlflow.org/docs/latest/python_api/mlflow.html) to learn more about the log_params, log_metrics and log_artifact functions. Find out how to log sklearn models [here](https://mlflow.org/docs/latest/python_api/mlflow.sklearn.html])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Complete the following in order to log the parameters, interesting metrics and the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/01/05 15:41:56 INFO mlflow.tracking.fluent: Experiment with name 'iris-experiment-1' does not exist. Creating a new experiment.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "default artifacts URI: 'mlflow-artifacts:/823888780486837844/eda23daad43e42e0a76ef762237536a4/artifacts'\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "mlflow.set_experiment(\"iris-experiment-1\")\n",
    "\n",
    "with mlflow.start_run() as run:\n",
    "    run_id = run.info.run_id\n",
    "\n",
    "    X, y = load_iris(return_X_y=True)\n",
    "\n",
    "    params = {\"C\": 0.1, \"random_state\": 42}\n",
    "    mlflow.log_params(params)\n",
    "\n",
    "    lr = LogisticRegression(**params).fit(X, y)\n",
    "    y_pred = lr.predict(X)\n",
    "    mlflow.log_metric(\"accuracy\", accuracy_score(y, y_pred))\n",
    "\n",
    "    mlflow.sklearn.log_model(lr, artifact_path=\"models\")\n",
    "    print(f\"default artifacts URI: '{mlflow.get_artifact_uri()}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<Experiment: artifact_location='mlflow-artifacts:/823888780486837844', creation_time=1736088116909, experiment_id='823888780486837844', last_update_time=1736088116909, lifecycle_stage='active', name='iris-experiment-1', tags={}>,\n",
       " <Experiment: artifact_location='mlflow-artifacts:/0', creation_time=1736088085839, experiment_id='0', last_update_time=1736088085839, lifecycle_stage='active', name='Default', tags={}>]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "experiments = client.search_experiments()\n",
    "experiments"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try running the training script with various parameters to have runs to compare.\n",
    "You can now explore your run(s) using the ui: \\\n",
    "(Paste \"mlflow ui --host 0.0.0.0 --port 5002\" in your terminal, or run the cell below)\n",
    "\n",
    "**N.B.** Make sure you are in the lecture folder and not the repo root!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "^C\n"
     ]
    }
   ],
   "source": [
    "#!mlflow ui --host 0.0.0.0 --port 5002"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You will have to kill the cell to continue experimenting"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interacting with the model registry"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you are satisfied with the last run's model, you can transform the logged model into a registered model. It will be logged in the Model Registry, which makes it easier to use in production and manage versions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Successfully registered model 'iris_lr_model'.\n",
      "2025/01/06 03:00:47 INFO mlflow.tracking._model_registry.client: Waiting up to 300 seconds for model version to finish creation. Model name: iris_lr_model, version 1\n",
      "Created version '1' of model 'iris_lr_model'.\n"
     ]
    }
   ],
   "source": [
    "# We already have our run id from above. Another way to get it is to use the client:\n",
    "# run_id = client.list_run_infos(experiment_id='1')[0].run_id\n",
    "\n",
    "result = mlflow.register_model(f\"runs:/{run_id}/models\", \"iris_lr_model\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use Case"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The project is *New York City Taxi trip duration prediction*. \\\n",
    "The goal is to use the available data in order to train a simple machine learning model\n",
    "to predict the trip duration based on **some input that can be available in production environment**.\n",
    "\n",
    "An ultimate goal for this use case can be to predict in real time trips durations (google-maps/waze itinerary like)\n",
    "but for simplicity, in this module, we assume that we need batch prediction. The data for which we need predictions\n",
    "will be stored in a file for ingestion in the trained model.\n",
    "\n",
    "The machine learning phase is mainly constituted by the following steps : \n",
    "- data processing\n",
    "- model training\n",
    "- model evaluation\n",
    "- prediction\n",
    "\n",
    "The data to use for this module can be downloaded from the [TLC Trip Record Data page](https://www.nyc.gov/site/tlc/about/tlc-trip-record-data.page).\n",
    "To complete this module, you will need 03 samples of data :\n",
    "- `sample 1 example` : yellow trip 2021-01 data (to train model)\n",
    "- `sample 2 example` : yellow trip 2021-02 data (to evaluate model)\n",
    "- `sample 3 example` : yellow trip 2021-03 data (for prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "from typing import List\n",
    "from scipy.sparse import csr_matrix"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0 - Download Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading...\n",
      "From: https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2021-01.parquet\n",
      "To: c:\\Users\\c\\10.9\\esilv-mlops-crashcourse-24\\lessons\\01-model-and-experiment-management\\data\\yellow_tripdata_2021-01.parquet\n",
      "100%|██████████| 21.7M/21.7M [00:00<00:00, 32.3MB/s]\n",
      "Downloading...\n",
      "From: https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2021-02.parquet\n",
      "To: c:\\Users\\c\\10.9\\esilv-mlops-crashcourse-24\\lessons\\01-model-and-experiment-management\\data\\yellow_tripdata_2021-02.parquet\n",
      "100%|██████████| 21.8M/21.8M [00:00<00:00, 31.8MB/s]\n",
      "Downloading...\n",
      "From: https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2021-03.parquet\n",
      "To: c:\\Users\\c\\10.9\\esilv-mlops-crashcourse-24\\lessons\\01-model-and-experiment-management\\data\\yellow_tripdata_2021-03.parquet\n",
      "100%|██████████| 30.0M/30.0M [00:00<00:00, 36.8MB/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'data/yellow_tripdata_2021-03.parquet'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gdown\n",
    "import os\n",
    "\n",
    "data_folder = \"data\"\n",
    "train_path = f\"{data_folder}/yellow_tripdata_2021-01.parquet\"\n",
    "test_path = f\"{data_folder}/yellow_tripdata_2021-02.parquet\"\n",
    "predict_path = f\"{data_folder}/yellow_tripdata_2021-03.parquet\"\n",
    "\n",
    "# Check whether the specified path exists or not\n",
    "isExist = os.path.exists(data_folder)\n",
    "if not isExist:\n",
    "    # Create a new directory because it does not exist\n",
    "    os.makedirs(data_folder)\n",
    "    print(f\"New directory {data_folder} created!\")\n",
    "\n",
    "gdown.download(\n",
    "    \"https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2021-01.parquet\",\n",
    "    train_path,\n",
    "    quiet=False,\n",
    ")\n",
    "gdown.download(\n",
    "    \"https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2021-02.parquet\",\n",
    "    test_path,\n",
    "    quiet=False,\n",
    ")\n",
    "gdown.download(\n",
    "    \"https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2021-03.parquet\",\n",
    "    predict_path,\n",
    "    quiet=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1 - Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>VendorID</th>\n",
       "      <th>tpep_pickup_datetime</th>\n",
       "      <th>tpep_dropoff_datetime</th>\n",
       "      <th>passenger_count</th>\n",
       "      <th>trip_distance</th>\n",
       "      <th>RatecodeID</th>\n",
       "      <th>store_and_fwd_flag</th>\n",
       "      <th>PULocationID</th>\n",
       "      <th>DOLocationID</th>\n",
       "      <th>payment_type</th>\n",
       "      <th>fare_amount</th>\n",
       "      <th>extra</th>\n",
       "      <th>mta_tax</th>\n",
       "      <th>tip_amount</th>\n",
       "      <th>tolls_amount</th>\n",
       "      <th>improvement_surcharge</th>\n",
       "      <th>total_amount</th>\n",
       "      <th>congestion_surcharge</th>\n",
       "      <th>airport_fee</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>2021-01-01 00:30:10</td>\n",
       "      <td>2021-01-01 00:36:12</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.10</td>\n",
       "      <td>1.0</td>\n",
       "      <td>N</td>\n",
       "      <td>142</td>\n",
       "      <td>43</td>\n",
       "      <td>2</td>\n",
       "      <td>8.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.3</td>\n",
       "      <td>11.80</td>\n",
       "      <td>2.5</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>2021-01-01 00:51:20</td>\n",
       "      <td>2021-01-01 00:52:19</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.20</td>\n",
       "      <td>1.0</td>\n",
       "      <td>N</td>\n",
       "      <td>238</td>\n",
       "      <td>151</td>\n",
       "      <td>2</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.3</td>\n",
       "      <td>4.30</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>2021-01-01 00:43:30</td>\n",
       "      <td>2021-01-01 01:11:06</td>\n",
       "      <td>1.0</td>\n",
       "      <td>14.70</td>\n",
       "      <td>1.0</td>\n",
       "      <td>N</td>\n",
       "      <td>132</td>\n",
       "      <td>165</td>\n",
       "      <td>1</td>\n",
       "      <td>42.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>8.65</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.3</td>\n",
       "      <td>51.95</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>2021-01-01 00:15:48</td>\n",
       "      <td>2021-01-01 00:31:01</td>\n",
       "      <td>0.0</td>\n",
       "      <td>10.60</td>\n",
       "      <td>1.0</td>\n",
       "      <td>N</td>\n",
       "      <td>138</td>\n",
       "      <td>132</td>\n",
       "      <td>1</td>\n",
       "      <td>29.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>6.05</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.3</td>\n",
       "      <td>36.35</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2</td>\n",
       "      <td>2021-01-01 00:31:49</td>\n",
       "      <td>2021-01-01 00:48:21</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.94</td>\n",
       "      <td>1.0</td>\n",
       "      <td>N</td>\n",
       "      <td>68</td>\n",
       "      <td>33</td>\n",
       "      <td>1</td>\n",
       "      <td>16.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>4.06</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.3</td>\n",
       "      <td>24.36</td>\n",
       "      <td>2.5</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   VendorID tpep_pickup_datetime tpep_dropoff_datetime  passenger_count  \\\n",
       "0         1  2021-01-01 00:30:10   2021-01-01 00:36:12              1.0   \n",
       "1         1  2021-01-01 00:51:20   2021-01-01 00:52:19              1.0   \n",
       "2         1  2021-01-01 00:43:30   2021-01-01 01:11:06              1.0   \n",
       "3         1  2021-01-01 00:15:48   2021-01-01 00:31:01              0.0   \n",
       "4         2  2021-01-01 00:31:49   2021-01-01 00:48:21              1.0   \n",
       "\n",
       "   trip_distance  RatecodeID store_and_fwd_flag  PULocationID  DOLocationID  \\\n",
       "0           2.10         1.0                  N           142            43   \n",
       "1           0.20         1.0                  N           238           151   \n",
       "2          14.70         1.0                  N           132           165   \n",
       "3          10.60         1.0                  N           138           132   \n",
       "4           4.94         1.0                  N            68            33   \n",
       "\n",
       "   payment_type  fare_amount  extra  mta_tax  tip_amount  tolls_amount  \\\n",
       "0             2          8.0    3.0      0.5        0.00           0.0   \n",
       "1             2          3.0    0.5      0.5        0.00           0.0   \n",
       "2             1         42.0    0.5      0.5        8.65           0.0   \n",
       "3             1         29.0    0.5      0.5        6.05           0.0   \n",
       "4             1         16.5    0.5      0.5        4.06           0.0   \n",
       "\n",
       "   improvement_surcharge  total_amount  congestion_surcharge  airport_fee  \n",
       "0                    0.3         11.80                   2.5          NaN  \n",
       "1                    0.3          4.30                   0.0          NaN  \n",
       "2                    0.3         51.95                   0.0          NaN  \n",
       "3                    0.3         36.35                   0.0          NaN  \n",
       "4                    0.3         24.36                   2.5          NaN  "
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def load_data(path: str):\n",
    "    return pd.read_parquet(path)\n",
    "\n",
    "\n",
    "train_df = load_data(train_path)\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2 - Prepare the data"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's prepare the data to make it Machine Learning ready. \\\n",
    "For this, we need to clean it, compute the target (what we want to predict), and compute some features to help the model understand the data better."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2-1 Compute the target"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to predict a taxi trip duration in minutes. Let's compute it as a difference between the drop-off time and the pick-up time for each trip."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_target(\n",
    "    df: pd.DataFrame,\n",
    "    pickup_column: str = \"tpep_pickup_datetime\",\n",
    "    dropoff_column: str = \"tpep_dropoff_datetime\",\n",
    ") -> pd.DataFrame:\n",
    "    df[\"duration\"] = df[dropoff_column] - df[pickup_column]\n",
    "    df[\"duration\"] = df[\"duration\"].dt.total_seconds() / 60\n",
    "    return df\n",
    "\n",
    "\n",
    "train_df = compute_target(train_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    1.369769e+06\n",
       "mean     1.391168e+01\n",
       "std      1.312006e+02\n",
       "min     -1.350846e+05\n",
       "25%      5.566667e+00\n",
       "50%      9.066667e+00\n",
       "75%      1.461667e+01\n",
       "max      2.881770e+04\n",
       "Name: duration, dtype: float64"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df[\"duration\"].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's remove outliers and reduce the scope to trips between 1 minute and 1 hour"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "MIN_DURATION = 1\n",
    "MAX_DURATION = 60\n",
    "\n",
    "\n",
    "def filter_outliers(\n",
    "    df: pd.DataFrame, min_duration: int = 1, max_duration: int = 60\n",
    ") -> pd.DataFrame:\n",
    "    return df[df[\"duration\"].between(min_duration, max_duration)]\n",
    "\n",
    "\n",
    "train_df = filter_outliers(train_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2-2 Prepare features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2-2-1 Categorical features"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Most machine learning models don't work with categorical features. Because of this, they must be transformed so that the ML model can consume them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "CATEGORICAL_COLS = [\"PUlocationID\", \"DOlocationID\"]\n",
    "\n",
    "\n",
    "def encode_categorical_cols(\n",
    "    df: pd.DataFrame, categorical_cols: List[str] = None\n",
    ") -> pd.DataFrame:\n",
    "    if categorical_cols is None:\n",
    "        categorical_cols = [\"PULocationID\", \"DOLocationID\", \"passenger_count\"]\n",
    "    df[categorical_cols] = df[categorical_cols].fillna(-1).astype(\"int\")\n",
    "    df[categorical_cols] = df[categorical_cols].astype(\"str\")\n",
    "    return df\n",
    "\n",
    "\n",
    "train_df = encode_categorical_cols(train_df)\n",
    "train_df = train_df[:50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_x_y(\n",
    "    df: pd.DataFrame,\n",
    "    categorical_cols: List[str] = None,\n",
    "    dv: DictVectorizer = None,\n",
    "    with_target: bool = True,\n",
    ") -> dict:\n",
    "\n",
    "    if categorical_cols is None:\n",
    "        categorical_cols = [\"PULocationID\", \"DOLocationID\", \"passenger_count\"]\n",
    "    dicts = df[categorical_cols].to_dict(orient=\"records\")\n",
    "\n",
    "    y = None\n",
    "    if with_target:\n",
    "        if dv is None:\n",
    "            dv = DictVectorizer()\n",
    "            dv.fit(dicts)\n",
    "        y = df[\"duration\"].values\n",
    "\n",
    "    x = dv.transform(dicts)\n",
    "    return x, y, dv\n",
    "\n",
    "\n",
    "X_train, y_train, dv = extract_x_y(train_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3 - Train model"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We train a basic linear regression model to have a baseline performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(x_train: csr_matrix, y_train: np.ndarray):\n",
    "    lr = LinearRegression()\n",
    "    lr.fit(x_train, y_train)\n",
    "    return lr\n",
    "\n",
    "\n",
    "model = train_model(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "def train_model_xgboost(x_train: csr_matrix, y_train: np.ndarray):\n",
    "    dtrain = xgb.DMatrix(x_train, label=y_train)\n",
    "    params = {\"objective\": \"reg:squarederror\", \"eval_metric\": \"rmse\"}\n",
    "    model = xgb.train(params, dtrain)\n",
    "    return model\n",
    "\n",
    "model_xgboost = train_model_xgboost(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4 - Evaluate model"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We evaluate the model on train and test data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4-1 On train data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.26000000000099904"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def predict_duration(input_data: csr_matrix, model: LinearRegression):\n",
    "    return model.predict(input_data)\n",
    "\n",
    "\n",
    "def evaluate_model(y_true: np.ndarray, y_pred: np.ndarray):\n",
    "    return mean_squared_error(y_true, y_pred, squared=False)\n",
    "\n",
    "\n",
    "prediction = predict_duration(X_train, model)\n",
    "train_me = evaluate_model(y_train, prediction)\n",
    "train_me"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "def load_pickle(path: str):\n",
    "    with open(path, \"rb\") as f:\n",
    "        loaded_obj = pickle.load(f)\n",
    "    return loaded_obj\n",
    "\n",
    "\n",
    "def predict_updated(input_path: str, model: LinearRegression):\n",
    "    input_data = load_pickle(input_path)\n",
    "    return model.predict(input_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3.477482658335206"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#model_xgboost = train_model_xgboost(X_train, y_train)\n",
    "dmatrix_data = xgb.DMatrix(X_train)\n",
    "y_pred_xgboost = model_xgboost.predict(dmatrix_data)\n",
    "\n",
    "train_me_xgboost = evaluate_model(y_train, y_pred_xgboost)\n",
    "train_me_xgboost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4-2 On test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = load_data(test_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = compute_target(test_df)\n",
    "test_df = encode_categorical_cols(test_df)\n",
    "X_test, y_test, _ = extract_x_y(test_df, dv=dv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "58.71506045956022"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred_test = predict_duration(X_test, model)\n",
    "test_me = evaluate_model(y_test, y_pred_test)\n",
    "test_me"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "58.51669053507327"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#y_pred_test_xgboost = predict_duration(X_test, model_xgboost)\n",
    "dmatrix_data = xgb.DMatrix(X_test)\n",
    "y_pred_test = model_xgboost.predict(dmatrix_data)\n",
    "test_me_xgboost = evaluate_model(y_test, y_pred_test)\n",
    "test_me_xgboost"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4 - Log Model Parameters to MlFlow"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that all our development function are built and tested, let's create a training pipeline and log the training parameters, logs and model to MlFlow."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a training flow, log all the important parameters, metrics and model. Try to find what could be important and needs to be logged."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Successfully registered model 'linear_reg_test'.\n",
      "2025/01/06 03:02:12 INFO mlflow.tracking._model_registry.client: Waiting up to 300 seconds for model version to finish creation. Model name: linear_reg_test, version 1\n",
      "Created version '1' of model 'linear_reg_test'.\n"
     ]
    }
   ],
   "source": [
    "# Set the experiment name\n",
    "mlflow_experiment_path = f\"/mlflow/linear_reg_test\"\n",
    "mlflow.set_experiment(mlflow_experiment_path)\n",
    "\n",
    "# Start a run\n",
    "with mlflow.start_run() as run:\n",
    "    run_id = run.info.run_id\n",
    "\n",
    "    # Set tags for the run\n",
    "    mlflow.set_tag(\"Level\", \"Development\")\n",
    "    mlflow.set_tag(\"Team\", \"Data Science\")\n",
    "\n",
    "    # Load data\n",
    "    train_df = load_data(train_path)\n",
    "    test_df = load_data(test_path)\n",
    "    mlflow.log_param(\n",
    "        \"train_date\", train_path.split(\"/\")[-1].split(\".\")[0].split(\"_\")[-1]\n",
    "    )\n",
    "    mlflow.log_param(\"test_date\", test_path.split(\"/\")[-1].split(\".\")[0].split(\"_\")[-1])\n",
    "    mlflow.log_param(\"train_set_size\", train_df.shape[0])\n",
    "    mlflow.log_param(\"test_set_size\", test_df.shape[0])\n",
    "\n",
    "    # Compute target\n",
    "    train_df_computed = compute_target(train_df)\n",
    "\n",
    "    # Filter outliers\n",
    "    mlflow.log_param(\"filtered_outliers\", True)\n",
    "    train_df_computed = filter_outliers(train_df_computed)\n",
    "\n",
    "    # Encode categorical columns\n",
    "    train_df = encode_categorical_cols(train_df)\n",
    "\n",
    "    # Extract X and y\n",
    "    X_train, y_train, _ = extract_x_y(train_df)\n",
    "\n",
    "    # Train model\n",
    "    model = train_model(X_train, y_train)\n",
    "\n",
    "    # Evaluate model\n",
    "    prediction = predict_duration(X_train, model)\n",
    "    train_me = evaluate_model(y_train, prediction)\n",
    "    mlflow.log_metric(\"train_me\", train_me)\n",
    "\n",
    "    # Evaluate model on test set\n",
    "    test_df = compute_target(test_df)\n",
    "    test_df = encode_categorical_cols(test_df)\n",
    "    # Train data\n",
    "    X_train, y_train, dv = extract_x_y(train_df)\n",
    "\n",
    "# Test data using the same dv\n",
    "   \n",
    "\n",
    "    X_test, y_test, dv = extract_x_y(test_df, dv=dv)\n",
    "    #dmatrix_data = xgb.DMatrix(X_test)\n",
    "    #y_pred_test = model_xgboost.predict(dmatrix_data)\n",
    "    y_pred_test = predict_duration(X_test,model)\n",
    "    test_me = evaluate_model(y_test, y_pred_test)\n",
    "    mlflow.log_metric(\"test_me\", test_me)\n",
    "\n",
    "    # Log your model\n",
    "    mlflow.sklearn.log_model(model, \"models\")\n",
    "\n",
    "    # Register your model as the production model\n",
    "    mlflow.register_model(f\"runs:/{run_id}/models\", \"linear_reg_test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/01/05 15:44:12 INFO mlflow.tracking.fluent: Experiment with name '/mlflow/xgboost_test' does not exist. Creating a new experiment.\n",
      "c:\\Users\\c\\anaconda3\\envs\\nyc-taxi\\lib\\site-packages\\_distutils_hack\\__init__.py:15: UserWarning: Distutils was imported before Setuptools, but importing Setuptools also replaces the `distutils` module in `sys.modules`. This may lead to undesirable behaviors or errors. To avoid these issues, avoid using distutils directly, ensure that setuptools is installed in the traditional way (e.g. not an editable install), and/or make sure that setuptools is always imported before distutils.\n",
      "  warnings.warn(\n",
      "c:\\Users\\c\\anaconda3\\envs\\nyc-taxi\\lib\\site-packages\\_distutils_hack\\__init__.py:30: UserWarning: Setuptools is replacing distutils. Support for replacing an already imported distutils is deprecated. In the future, this condition will fail. Register concerns at https://github.com/pypa/setuptools/issues/new?template=distutils-deprecation.yml\n",
      "  warnings.warn(\n",
      "Successfully registered model 'xgboost_test'.\n",
      "2025/01/05 15:44:39 INFO mlflow.tracking._model_registry.client: Waiting up to 300 seconds for model version to finish creation. Model name: xgboost_test, version 1\n",
      "Created version '1' of model 'xgboost_test'.\n"
     ]
    }
   ],
   "source": [
    "# Set the experiment name\n",
    "mlflow_experiment_path = f\"/mlflow/xgboost_test\"\n",
    "mlflow.set_experiment(mlflow_experiment_path)\n",
    "\n",
    "# Start a run\n",
    "with mlflow.start_run() as run:\n",
    "    run_id = run.info.run_id\n",
    "\n",
    "    # Set tags for the run\n",
    "    mlflow.set_tag(\"Level\", \"Development\")\n",
    "    mlflow.set_tag(\"Team\", \"Data Science\")\n",
    "\n",
    "    # Load data\n",
    "    train_df = load_data(train_path)\n",
    "    test_df = load_data(test_path)\n",
    "    mlflow.log_param(\n",
    "        \"train_date\", train_path.split(\"/\")[-1].split(\".\")[0].split(\"_\")[-1]\n",
    "    )\n",
    "    mlflow.log_param(\"test_date\", test_path.split(\"/\")[-1].split(\".\")[0].split(\"_\")[-1])\n",
    "    mlflow.log_param(\"train_set_size\", train_df.shape[0])\n",
    "    mlflow.log_param(\"test_set_size\", test_df.shape[0])\n",
    "\n",
    "    # Compute target\n",
    "    train_df_computed = compute_target(train_df)\n",
    "\n",
    "    # Filter outliers\n",
    "    mlflow.log_param(\"filtered_outliers\", True)\n",
    "    train_df_computed = filter_outliers(train_df_computed)\n",
    "\n",
    "    # Encode categorical columns\n",
    "    train_df = encode_categorical_cols(train_df)\n",
    "\n",
    "    # Extract X and y\n",
    "    X_train, y_train, _ = extract_x_y(train_df)\n",
    "\n",
    "    # Train model\n",
    "    model_xgboost = train_model(X_train, y_train)\n",
    "\n",
    "    # Evaluate model\n",
    "    prediction = predict_duration(X_train, model_xgboost)\n",
    "    train_me = evaluate_model(y_train, prediction)\n",
    "    mlflow.log_metric(\"train_me\", train_me)\n",
    "\n",
    "    # Evaluate model on test set\n",
    "    test_df = compute_target(test_df)\n",
    "    test_df = encode_categorical_cols(test_df)\n",
    "    X_train, y_train, dv = extract_x_y(train_df)\n",
    "    X_test, y_test, dv = extract_x_y(test_df, dv=dv)\n",
    "    dmatrix_data = xgb.DMatrix(X_test)\n",
    "    y_pred_test = model_xgboost.predict(X_test)  # X_test should be a numpy array or sparse matrix\n",
    "\n",
    "    #y_pred_test = model_xgboost.predict(dmatrix_data)\n",
    "    #y_pred_test = model_xgboost.predict(dmatrix_data)\n",
    "    #y_pred_test = predict_duration(X_test, model)\n",
    "    test_me = evaluate_model(y_test, y_pred_test)\n",
    "    mlflow.log_metric(\"test_me\", test_me)\n",
    "\n",
    "    # Log your model\n",
    "    mlflow.sklearn.log_model(model, \"models\")\n",
    "\n",
    "    # Register your model as the production model\n",
    "    mlflow.register_model(f\"runs:/{run_id}/models\", \"xgboost_test\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If the model is satisfactory, we stage it as production using the appropriate version. This will help us retreiving it for predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<RegisteredModel: aliases={}, creation_timestamp=1736132869984, description='', last_updated_timestamp=1736133029433, latest_versions=[<ModelVersion: aliases=[], creation_timestamp=1736133029433, current_stage='None', description='', last_updated_timestamp=1736133029433, name='RandomForestRegressor_test', run_id='ec311e061e204c28a60c9aca47cfa407', run_link='', source='mlflow-artifacts:/958600936449629552/ec311e061e204c28a60c9aca47cfa407/artifacts/models', status='READY', status_message='', tags={}, user_id='', version='2'>], name='RandomForestRegressor_test', tags={}>\n",
      "<RegisteredModel: aliases={}, creation_timestamp=1736128847621, description='', last_updated_timestamp=1736128847649, latest_versions=[<ModelVersion: aliases=[], creation_timestamp=1736128847649, current_stage='None', description='', last_updated_timestamp=1736128847649, name='iris_lr_model', run_id='d77f733e1e5140328c7962740bcf7778', run_link='', source='mlflow-artifacts:/718290349955982801/d77f733e1e5140328c7962740bcf7778/artifacts/models', status='READY', status_message='', tags={}, user_id='', version='1'>], name='iris_lr_model', tags={}>\n",
      "<RegisteredModel: aliases={}, creation_timestamp=1736128931966, description='', last_updated_timestamp=1736133090498, latest_versions=[<ModelVersion: aliases=[], creation_timestamp=1736132788062, current_stage='None', description='', last_updated_timestamp=1736132788062, name='linear_reg_test', run_id='8dc4fade815c412786cd508745fdbd71', run_link='', source='mlflow-artifacts:/958600936449629552/8dc4fade815c412786cd508745fdbd71/artifacts/models', status='READY', status_message='', tags={}, user_id='', version='2'>,\n",
      " <ModelVersion: aliases=[], creation_timestamp=1736128931993, current_stage='Production', description='', last_updated_timestamp=1736133090498, name='linear_reg_test', run_id='38cb6da02b334c2b9ea53a36383f49d1', run_link='', source='mlflow-artifacts:/718290349955982801/38cb6da02b334c2b9ea53a36383f49d1/artifacts/models', status='READY', status_message='', tags={}, user_id='', version='1'>], name='linear_reg_test', tags={}>\n",
      "<RegisteredModel: aliases={}, creation_timestamp=1736121793650, description='', last_updated_timestamp=1736122044016, latest_versions=[<ModelVersion: aliases=[], creation_timestamp=1736122044016, current_stage='None', description='', last_updated_timestamp=1736122044016, name='tracking-diamonds', run_id='772dbda42e2f48d5872c07c3079f074a', run_link='', source=('mlflow-artifacts:/936627379886763873/772dbda42e2f48d5872c07c3079f074a/artifacts/Support '\n",
      " 'Vector Regression_model'), status='READY', status_message='', tags={}, user_id='', version='4'>], name='tracking-diamonds', tags={}>\n",
      "Version: 2, Stage: None\n",
      "Version: 1, Stage: Production\n"
     ]
    }
   ],
   "source": [
    "from mlflow.tracking import MlflowClient\n",
    "\n",
    "client = MlflowClient()\n",
    "\n",
    "# List registered models\n",
    "for model in client.search_registered_models():\n",
    "    print(model)\n",
    "\n",
    "# List all versions for the specific model\n",
    "model_name = \"linear_reg_test\"\n",
    "model_versions = client.get_registered_model(name=model_name)\n",
    "for version in model_versions.latest_versions:\n",
    "    print(f\"Version: {version.version}, Stage: {version.current_stage}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<ModelVersion: aliases=[], creation_timestamp=1736128931993, current_stage='Production', description='', last_updated_timestamp=1736133189437, name='linear_reg_test', run_id='38cb6da02b334c2b9ea53a36383f49d1', run_link='', source='mlflow-artifacts:/718290349955982801/38cb6da02b334c2b9ea53a36383f49d1/artifacts/models', status='READY', status_message='', tags={}, user_id='', version='1'>"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "client = MlflowClient()\n",
    "production_version = 1\n",
    "\n",
    "client.transition_model_version_stage(\n",
    "    name=\"linear_reg_test\", version=production_version, stage=\"Production\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<ModelVersion: aliases=[], creation_timestamp=1736088279758, current_stage='Production', description='', last_updated_timestamp=1736088279963, name='xgboost_test', run_id='16d7df5f0b1144c08e784201a0ea5996', run_link='', source='mlflow-artifacts:/775980936993477273/16d7df5f0b1144c08e784201a0ea5996/artifacts/models', status='READY', status_message='', tags={}, user_id='', version='1'>"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "client = MlflowClient()\n",
    "production_version = 1\n",
    "\n",
    "client.transition_model_version_stage(\n",
    "    name=\"xgboost_test\", version=production_version, stage=\"Production\"\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5 - Predict"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now use our model to predict on fresh unseen data and forecast what is going to be the duration of a taxi trip depending on trip characteristics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\c\\anaconda3\\envs\\nyc-taxi\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Downloading artifacts: 100%|██████████| 5/5 [00:00<00:00, 99.61it/s] \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([12.7069711 , 13.81109596, 13.81109596, ..., 14.1538733 ,\n",
       "       20.51853691, 24.14175089])"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load prediction data\n",
    "predict_df = load_data(predict_path)\n",
    "\n",
    "# Apply feature engineering\n",
    "predict_df = encode_categorical_cols(predict_df)\n",
    "X_pred, _, dv2 = extract_x_y(predict_df, dv=dv, with_target=False)\n",
    "\n",
    "# Load production model\n",
    "model_uri = f\"models:/{mlflow_experiment_path}/production\"\n",
    "model = mlflow.sklearn.load_model(model_uri)\n",
    "\n",
    "# Make predictions\n",
    "y_pred = predict_duration(X_pred, model)\n",
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Any' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[46], line 4\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mos\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpickle\u001b[39;00m\n\u001b[1;32m----> 4\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msave_pickle\u001b[39m(path: \u001b[38;5;28mstr\u001b[39m, obj: \u001b[43mAny\u001b[49m):\n\u001b[0;32m      5\u001b[0m     \u001b[38;5;66;03m# Ensure the directory exists\u001b[39;00m\n\u001b[0;32m      6\u001b[0m     dir_name \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mdirname(path)\n\u001b[0;32m      7\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mexists(dir_name):\n",
      "\u001b[1;31mNameError\u001b[0m: name 'Any' is not defined"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pickle\n",
    "\n",
    "def save_pickle(path: str, obj: Any):\n",
    "    # Ensure the directory exists\n",
    "    dir_name = os.path.dirname(path)\n",
    "    if not os.path.exists(dir_name):\n",
    "        os.makedirs(dir_name)\n",
    "    \n",
    "    # Save the pickle file\n",
    "    with open(path, \"wb\") as f:\n",
    "        pickle.dump(obj, f)\n",
    "\n",
    "# Call the function to save the pickle\n",
    "save_pickle(\"../../02-model-deployment/solution/web_service/local_models/dv__v0.0.1.pkl\", dv2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DictVectorizer()\n"
     ]
    }
   ],
   "source": [
    "loaded_dv2 = load_pickle(\"../../02-model-deployment/solution/web_service/local_models/dv__v0.0.1.pkl\")\n",
    "print(loaded_dv2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current working directory: c:\\Users\\c\\10.9\\esilv-mlops-crashcourse-24\\lessons\\02-model-deployment\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "print(f\"Current working directory: {os.getcwd()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c:\\Users\\c\\10.9\\esilv-mlops-crashcourse-24\\lessons\\02-model-deployment\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\c\\anaconda3\\envs\\nyc-taxi\\lib\\site-packages\\IPython\\core\\magics\\osm.py:417: UserWarning: This is now an optional IPython functionality, setting dhist requires you to install the `pickleshare` library.\n",
      "  self.shell.db['dhist'] = compress_dhist(dhist)[-100:]\n"
     ]
    }
   ],
   "source": [
    "cd 02-model-deployment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "from typing import Any\n",
    "\n",
    "def save_pickle(path: str, obj: Any):\n",
    "    \"\"\"Saves the given object to a pickle file.\"\"\"\n",
    "    with open(path, \"wb\") as f:\n",
    "        pickle.dump(obj, f)\n",
    "\n",
    "def load_pickle(path: str):\n",
    "    \"\"\"Loads a pickle object from the specified file.\"\"\"\n",
    "    with open(path, \"rb\") as f:\n",
    "        return pickle.load(f)\n",
    "save_pickle('web_service/local_models/dv__v0.0.1.pkl', dv)  # Save the DictVectorizer\n",
    "save_pickle('web_service/local_models/model__v0.0.1.pkl', model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'dv2' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 17\u001b[0m\n\u001b[0;32m     14\u001b[0m         pickle\u001b[38;5;241m.\u001b[39mdump(obj, f)\n\u001b[0;32m     16\u001b[0m \u001b[38;5;66;03m# Example usage: Save the pickle file to the correct path\u001b[39;00m\n\u001b[1;32m---> 17\u001b[0m save_pickle(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mesilv-mlops-crashcourse-24/lessons/02-model-deployment/web_service/local_models/dv__v0.0.1.pkl\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[43mdv2\u001b[49m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'dv2' is not defined"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pickle\n",
    "from typing import Any\n",
    "\n",
    "\n",
    "\n",
    "def save_pickle(path: str, obj: Any):\n",
    "    # Ensure the directory exists\n",
    "    dir_name = os.path.dirname(path)\n",
    "    if not os.path.exists(dir_name):\n",
    "        os.makedirs(dir_name)  # Creates all intermediate directories if they don't exist\n",
    "    \n",
    "    # Save the pickle file\n",
    "    with open(path, \"wb\") as f:\n",
    "        pickle.dump(obj, f)\n",
    "\n",
    "# Example usage: Save the pickle file to the correct path\n",
    "save_pickle(\"esilv-mlops-crashcourse-24/lessons/02-model-deployment/web_service/local_models/dv__v0.0.1.pkl\", dv2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'lib.utils'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Saving the preprocessor\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m save_pickle\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# Assuming dv2 is your DictVectorizer\u001b[39;00m\n\u001b[0;32m      5\u001b[0m save_pickle(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlocal_models/dv__v0.0.1.pkl\u001b[39m\u001b[38;5;124m\"\u001b[39m, dv2)\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'lib.utils'"
     ]
    }
   ],
   "source": [
    "# Saving the preprocessor\n",
    "from lib.utils import save_pickle\n",
    "\n",
    "# Assuming dv2 is your DictVectorizer\n",
    "save_pickle(\"local_models/dv__v0.0.1.pkl\", dv2)\n",
    "\n",
    "# Saving the model\n",
    "save_pickle(\"local_models/model__v0.0.1.pkl\", model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'esilv-mlops-crashcourse-24/lessons/02-model-deployment/web_service/local_models/dv__v0.0.1.pkl'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[50], line 15\u001b[0m\n\u001b[0;32m     12\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(path, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwb\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[0;32m     13\u001b[0m         pickle\u001b[38;5;241m.\u001b[39mdump(obj, f)\n\u001b[1;32m---> 15\u001b[0m \u001b[43msave_pickle\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mesilv-mlops-crashcourse-24/lessons/02-model-deployment/web_service/local_models/dv__v0.0.1.pkl\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdv2\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     17\u001b[0m \u001b[38;5;66;03m############################################\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[50], line 12\u001b[0m, in \u001b[0;36msave_pickle\u001b[1;34m(path, obj)\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msave_pickle\u001b[39m(path: \u001b[38;5;28mstr\u001b[39m, obj: Any):\n\u001b[1;32m---> 12\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mwb\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[0;32m     13\u001b[0m         pickle\u001b[38;5;241m.\u001b[39mdump(obj, f)\n",
      "File \u001b[1;32mc:\\Users\\c\\anaconda3\\envs\\nyc-taxi\\lib\\site-packages\\IPython\\core\\interactiveshell.py:324\u001b[0m, in \u001b[0;36m_modified_open\u001b[1;34m(file, *args, **kwargs)\u001b[0m\n\u001b[0;32m    317\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m file \u001b[38;5;129;01min\u001b[39;00m {\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m}:\n\u001b[0;32m    318\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    319\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIPython won\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt let you open fd=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m by default \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    320\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mas it is likely to crash IPython. If you know what you are doing, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    321\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124myou can use builtins\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m open.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    322\u001b[0m     )\n\u001b[1;32m--> 324\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m io_open(file, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'esilv-mlops-crashcourse-24/lessons/02-model-deployment/web_service/local_models/dv__v0.0.1.pkl'"
     ]
    }
   ],
   "source": [
    "\n",
    "############################################\n",
    "from typing import Any\n",
    "import pickle\n",
    "\n",
    "def load_pickle(path: str):\n",
    "    with open(path, \"rb\") as f:\n",
    "        loaded_obj = pickle.load(f)\n",
    "    return loaded_obj\n",
    "\n",
    "\n",
    "\n",
    "def save_pickle(path: str, obj: Any):\n",
    "    with open(path, \"wb\") as f:\n",
    "        pickle.dump(obj, f)\n",
    "\n",
    "save_pickle(\"esilv-mlops-crashcourse-24/lessons/02-model-deployment/web_service/local_models/dv__v0.0.1.pkl\", dv2)\n",
    "\n",
    "############################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Any\n",
    "\n",
    "import pickle\n",
    "\n",
    "\n",
    "def load_pickle(path: str):\n",
    "    with open(path, \"rb\") as f:\n",
    "        loaded_obj = pickle.load(f)\n",
    "    return loaded_obj\n",
    "\n",
    "\n",
    "def save_pickle(path: str, obj: Any):\n",
    "    with open(path, \"wb\") as f:\n",
    "        pickle.dump(obj, f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nyc-taxi",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
