{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning Pipelines and Orchestration with Prefect"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ‚ö†Ô∏è  **Version**: This module has been created using Prefect 2.13.7"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0 - Useful functions"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0.1 - From previous lessons\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lib/config.py\n",
    "CATEGORICAL_COLS = [\"PULocationID\", \"DOLocationID\", \"passenger_count\"]\n",
    "\n",
    "DATA_DIRPATH = \"../../data\"\n",
    "MODELS_DIRPATH = \"../../models\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lib/preprocessing.py\n",
    "from typing import List, Tuple\n",
    "\n",
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy.sparse\n",
    "from loguru import logger\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "\n",
    "\n",
    "def compute_target(\n",
    "    df: pd.DataFrame, pickup_column: str = \"tpep_pickup_datetime\", dropoff_column: str = \"tpep_dropoff_datetime\"\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"Compute the trip duration in minutes based on pickup and dropoff time\"\"\"\n",
    "    df[\"duration\"] = df[dropoff_column] - df[pickup_column]\n",
    "    df[\"duration\"] = df[\"duration\"].dt.total_seconds() / 60\n",
    "    return df\n",
    "\n",
    "\n",
    "def filter_outliers(df: pd.DataFrame, min_duration: int = 1, max_duration: int = 60) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Remove rows corresponding to negative/zero\n",
    "    and too high target' values from the dataset\n",
    "    \"\"\"\n",
    "    return df[df[\"duration\"].between(min_duration, max_duration)]\n",
    "\n",
    "\n",
    "def encode_categorical_cols(df: pd.DataFrame, categorical_cols: List[str] = None) -> pd.DataFrame:\n",
    "    \"\"\"Encode categorical columns as strings\"\"\"\n",
    "    if categorical_cols is None:\n",
    "        categorical_cols = CATEGORICAL_COLS\n",
    "    df.loc[:, categorical_cols] = df[categorical_cols].fillna(-1).astype(\"int\")\n",
    "    df.loc[:, categorical_cols] = df[categorical_cols].astype(\"str\")\n",
    "    return df\n",
    "\n",
    "\n",
    "def extract_x_y(\n",
    "    df: pd.DataFrame,\n",
    "    categorical_cols: List[str] = None,\n",
    "    dv: DictVectorizer = None,\n",
    "    with_target: bool = True,\n",
    ") -> Tuple[scipy.sparse.csr_matrix, np.ndarray, DictVectorizer]:\n",
    "    \"\"\"Extract X and y from the dataframe\"\"\"\n",
    "    if categorical_cols is None:\n",
    "        categorical_cols = CATEGORICAL_COLS\n",
    "    dicts = df[categorical_cols].to_dict(orient=\"records\")\n",
    "\n",
    "    y = None\n",
    "    if with_target:\n",
    "        if dv is None:\n",
    "            dv = DictVectorizer()\n",
    "            dv.fit(dicts)\n",
    "        y = df[\"duration\"].values\n",
    "\n",
    "    x = dv.transform(dicts)\n",
    "    return x, y, dv\n",
    "\n",
    "\n",
    "def process_data(filepath: str, dv=None, with_target: bool = True) -> scipy.sparse.csr_matrix:\n",
    "    \"\"\"\n",
    "    Load data from a parquet file\n",
    "    Compute target (duration column) and apply threshold filters (optional)\n",
    "    Turn features to sparce matrix\n",
    "    :return The sparce matrix, the target' values and the\n",
    "    dictvectorizer object if needed.\n",
    "    \"\"\"\n",
    "    df = pd.read_parquet(filepath)\n",
    "    if with_target:\n",
    "        logger.debug(f\"{filepath} | Computing target...\")\n",
    "        df1 = compute_target(df)\n",
    "        logger.debug(f\"{filepath} | Filtering outliers...\")\n",
    "        df2 = filter_outliers(df1)\n",
    "        logger.debug(f\"{filepath} | Encoding categorical columns...\")\n",
    "        df3 = encode_categorical_cols(df2)\n",
    "        logger.debug(f\"{filepath} | Extracting X and y...\")\n",
    "        return extract_x_y(df3, dv=dv)\n",
    "    else:\n",
    "        logger.debug(f\"{filepath} | Encoding categorical columns...\")\n",
    "        df1 = encode_categorical_cols(df)\n",
    "        logger.debug(f\"{filepath} | Extracting X and y...\")\n",
    "        return extract_x_y(df1, dv=dv, with_target=with_target)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0-2 Helpers for this session"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You also have other helpers to show you prefect's features in the `helpers.py` file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lib/helpers.py\n",
    "from typing import Any\n",
    "\n",
    "import os\n",
    "import pickle\n",
    "\n",
    "\n",
    "def load_pickle(path: str):\n",
    "    with open(path, \"rb\") as f:\n",
    "        loaded_obj = pickle.load(f)\n",
    "    return loaded_obj\n",
    "\n",
    "\n",
    "def save_pickle(path: str, obj: Any):\n",
    "    with open(path, \"wb\") as f:\n",
    "        pickle.dump(obj, f)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 - Create workflow functions"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create five functions to complete the ML process :\n",
    "- `train_model`\n",
    "- `predict`\n",
    "- `evaluate_model`\n",
    "- A workflow function to perform the whole training process `train_model_workflow`\n",
    "    - Process data\n",
    "    - Train model\n",
    "    - Evaluate model\n",
    "- A workflow function to perform the whole prediction process `batch_predict_workflow`\n",
    "    - Process data without target column\n",
    "    - Predict\n",
    "\n",
    "\n",
    "For the last two functions, you can start without saving / loading artifacts add these steps after.\n",
    "Please think about what artifacts you'll need to save and load to pass from training to predict workflows.\n",
    "\n",
    "Start by coding these functions here in the notebook\n",
    "\n",
    "Then, test your code with the downloaded data (e.g. January to train and February to predict).\n",
    "\n",
    "Finally, copy your code in the `lib` folder in the `modeling.py` and `workflows.py` files and test your workflows again using such a command:\n",
    "\n",
    "```bash\n",
    "python lib/workflows.py\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from typing import Optional\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy.sparse\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "\n",
    "def train_model(X: scipy.sparse.csr_matrix, y: np.ndarray) -> LinearRegression:\n",
    "    \"\"\"...\"\"\"\n",
    "    ...\n",
    "\n",
    "\n",
    "def predict(X: scipy.sparse.csr_matrix, model: LinearRegression) -> np.ndarray:\n",
    "    \"\"\"...\"\"\"\n",
    "    ...\n",
    "\n",
    "\n",
    "def evaluate_model(y_true: np.ndarray, y_pred: np.ndarray) -> float:\n",
    "    \"\"\"...\"\"\"\n",
    "    ...\n",
    "\n",
    "\n",
    "def train_model_workflow(\n",
    "    train_filepath: str,\n",
    "    test_filepath: str,\n",
    "    artifacts_filepath: Optional[str] = None,\n",
    ") -> dict:\n",
    "    \"\"\"...\"\"\"\n",
    "    ...\n",
    "\n",
    "\n",
    "def batch_predict_workflow(\n",
    "    input_filepath: str,\n",
    "    model: Optional[LinearRegression] = None,\n",
    "    dv: Optional[DictVectorizer] = None,\n",
    "    artifacts_filepath: Optional[str] = None,\n",
    ") -> np.ndarray:\n",
    "    \"\"\"...\"\"\"\n",
    "    ..."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 - Setup and explore Prefect\n",
    "\n",
    "We are going to use [Prefect](https://docs.prefect.io/2.6/tutorials/first-steps/), an Open Source orchestration tool with a Python SDK.\n",
    "\n",
    "\n",
    "**WINDOWS USERS**:\n",
    "\n",
    "You might run into issues with Prefect on Windows. If you do, please follow [Prefects instructions](https://docs.prefect.io/2.13.7/getting-started/installation/#install-prefect) to install Prefect on your machine"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2-1 Setup Prefect UI"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before starting to implement tasks and flows with prefect, let's set up the UI in order to have a good visualization of our work.\n",
    "\n",
    "Steps :\n",
    "\n",
    "- Set an API URL for your local server to make sure that your workflow will be tracked by this specific instance :\n",
    "```\n",
    "prefect config set PREFECT_API_URL=http://0.0.0.0:4200/api\n",
    "```\n",
    "\n",
    "- Check you have SQLite installed ([Prefect backend database system](https://docs.prefect.io/2.13.7/getting-started/installation/#external-requirements)):\n",
    "```\n",
    "sqlite3 --version \n",
    "```\n",
    "\n",
    "- Start a local prefect server :\n",
    "```\n",
    "prefect server start --host 0.0.0.0\n",
    "```\n",
    "\n",
    "If you want to reset the database, run :\n",
    "```\n",
    "prefect server database reset\n",
    "```\n",
    "\n",
    "\n",
    "You can visit the UI at http://0.0.0.0:4200/dashboard\n",
    "\n",
    "![](images/starting_page.png)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2-2 Prefect tasks and flows"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Prefect uses tasks and flows to build workflows](https://docs.prefect.io/2.13.7/tutorial/flows/).\n",
    "- Flows are like functions. They can take inputs, perform work, and return an output. In fact, you can turn any function into a Prefect flow by adding the @flow decorator\n",
    "- A task is any Python function decorated with a @task decorator called within a flow. You can think of a flow as a recipe for connecting a known sequence of tasks together. Tasks, and the dependencies between them, are displayed in the flow run graph, enabling you to break down a complex flow into something you can observe, understand and control at a more granular level.\n",
    "    - All tasks must be called from within a flow. Tasks may not call other tasks directly.\n",
    "    - Not all functions in a flow need be tasks. Use them only when their features are useful."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example\n",
    "import httpx\n",
    "from prefect import flow, task\n",
    "\n",
    "\n",
    "@task\n",
    "def get_url(url: str, params: dict = None):\n",
    "    response = httpx.get(url, params=params)\n",
    "    response.raise_for_status()\n",
    "    return response.json()\n",
    "\n",
    "\n",
    "@flow(retries=3, retry_delay_seconds=5, log_prints=True)\n",
    "def get_repo_info(repo_name: str = \"PrefectHQ/prefect\"):\n",
    "    url = f\"https://api.github.com/repos/{repo_name}\"\n",
    "    repo_stats = get_url(url)\n",
    "    print(f\"{repo_name} repository statistics ü§ì:\")\n",
    "    print(f\"Stars üå† : {repo_stats['stargazers_count']}\")\n",
    "    print(f\"Forks üç¥ : {repo_stats['forks_count']}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3 - Create Prefect tasks and flows"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3-1 Create tasks and flows"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the decorators `@task` and `@flow` to create your first prefect flow : The Processing flow.\n",
    "\n",
    "Prefect will try to use by default different thread to run each task. If you want sequential steps, introduce this dependencies through the name of each task output.\n",
    "\n",
    "\n",
    "Steps:\n",
    "- Create a task for each function you created in the previous section. You can start by doing these in the notebook.\n",
    "- Test your code by calling the flows run with downloaded data (this can be done in the notebook too).\n",
    "- Update your files in the `lib` folder. You should now have completed all files except `deployment.py`.\n",
    "\n",
    "\n",
    "You can see registered flows in the UI :\n",
    "![Flows in Prefect UI](images/flows_ui.png)\n",
    "\n",
    "\n",
    "And visualize the run of a flow :\n",
    "![Flows in Prefect UI](images/flow_run.png)\n",
    "\n",
    "\n",
    "> [!Warning]\n",
    "> **Typing tasks and flows in prefect** :\n",
    "> Typing tasks in prefect is done as with any python code.\n",
    "> For flows, either use `validate_parameters=False` or define pydantic models for prefect to understand your NON DEFAULT typing (see extra section).\n",
    "> But if all tasks are typed, since flows are just set of tasks, it should be all good if we don't want to add a layer of complexity\n",
    "> `Default types` : str, int ...\n",
    "\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3-2 Customize your flows"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can configure the properties and special behavior for your prefect tasks/flow in the decorator.\n",
    "For example, you can tell if you want to retry on a failure, set name or tags, etc... \\\n",
    "An example is given in the `helpers.py` file.\n",
    "```\n",
    "@task('name=failure_task', tags=['fails'], retries=3, retry_delay_seconds=60)\n",
    "def func():\n",
    "  ...\n",
    "\n",
    "```\n",
    "\n",
    "- Add names, tasks, and desired behavior to your tasks/flows\n",
    "- Test your code\n",
    "- Visualize in the local prefect UI\n",
    "\n",
    "If a task fails in the flow, it is possible to visualize which task fail and access the full log and traceback error\n",
    "by clicking on the tasks. \\\n",
    "We can also access run information inside de `state` object that can be returned by the flows using python code"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4 - Deploy your flows"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that all the workflows are defined, we can now schedule automatics runs for these pipelines.\n",
    "Let's assume that we have a process that tells us that our model need to be retrained weekly based on some performance analysis. We also receive data to predict each hour.\n",
    "\n",
    "Use prefect deployment object in order to :\n",
    "- Schedule complete ml process to run weekly\n",
    "- Schedule prediction pipeline to run each hour\n",
    "\n",
    "\n",
    "**Please note that you can test your code with the `to_deployment` here, however you'll have to move to scripts to test the deployment with `serve`.**\n",
    "\n",
    "You can deploy your flows by following [Prefect documentation here](https://docs.prefect.io/2.13.7/tutorial/deployments/#running-multiple-deployments-at-once).\n",
    "\n",
    "‚ö†Ô∏è  Serving a model with prefect is a long-running command, meaning that you will need to run it in a separate terminal or in the background.\n",
    "Interupting the command will stop the deployment, but you'll be still be able to see it the UI.\n",
    "\n",
    "In the UI, you should be able see deployments:\n",
    "![Deployments in Prefect UI](images/deployments.png)\n",
    "\n",
    "\n",
    "And the scheduled runs for one deployment:\n",
    "![Scheduled runs in Prefect UI](images/scheduled_runs.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hello_world.py\n",
    "from prefect import flow, serve\n",
    "\n",
    "\n",
    "@flow(name=\"Hello world\")\n",
    "def hello_world(name: str = \"world\"):\n",
    "    print(f\"Hello {name}!\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    hello_world_deployment = hello_world.to_deployment(\n",
    "        name='Hello world Deployment',\n",
    "        version='0.1.0',\n",
    "        tags=['hello world'],\n",
    "        interval=600\n",
    "        parameters={\n",
    "            'name': 'John Doe'\n",
    "        }\n",
    "    )\n",
    "    # Above: can be tested in notebook. Below: must be called from python script\n",
    "    serve(hello_world_deployment)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5 - Extra concepts"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5-1 Prefect workers"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5-2 Prefect typing using Pydantic"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mlops-crash-test",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
